{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHqOF9gaepK6",
        "outputId": "867a788d-6325-46bc-d8b6-25f620e79516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.56 🚀 Python-3.9.16 torch-1.13.1+cu116 CPU\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 27.3/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8n.pt')\n",
        "model.predict(\n",
        "   source='/content/frame0.jpg',\n",
        "   conf=0.25\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMFY9aHCez9p",
        "outputId": "ccaea64e-18c4-40da-d8f8-3f0da9fb2dd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/frame0.jpg: 384x640 1 person, 8 cars, 2 trucks, 183.4ms\n",
            "Speed: 1.3ms preprocess, 183.4ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{ '_keys': ('boxes', 'masks', 'probs'),\n",
              "   'boxes': ultralytics.yolo.engine.results.Boxes\n",
              " type:  torch.Tensor\n",
              " shape: torch.Size([11, 6])\n",
              " dtype: torch.float32\n",
              " tensor([[4.70539e+02, 1.98239e+02, 7.48001e+02, 3.34906e+02, 8.09065e-01, 2.00000e+00],\n",
              "         [1.35436e+03, 7.45324e+02, 1.67117e+03, 9.24393e+02, 7.39450e-01, 2.00000e+00],\n",
              "         [2.64216e-01, 1.86608e+02, 1.63679e+02, 3.47236e+02, 7.37963e-01, 7.00000e+00],\n",
              "         [1.51286e+03, 1.21667e+02, 1.67096e+03, 2.15675e+02, 7.11206e-01, 2.00000e+00],\n",
              "         [2.30772e+02, 2.37012e+02, 5.31634e+02, 4.24054e+02, 6.35892e-01, 2.00000e+00],\n",
              "         [1.19852e+03, 1.32631e+02, 1.41800e+03, 2.41039e+02, 5.75874e-01, 2.00000e+00],\n",
              "         [5.41271e+02, 3.44478e+02, 7.61275e+02, 6.50658e+02, 5.68950e-01, 2.00000e+00],\n",
              "         [5.53626e+02, 6.83165e+02, 1.63067e+03, 9.21341e+02, 4.64177e-01, 2.00000e+00],\n",
              "         [8.02890e+02, 0.00000e+00, 8.27719e+02, 5.42092e+01, 3.87353e-01, 0.00000e+00],\n",
              "         [5.41415e+02, 3.46836e+02, 7.60681e+02, 6.49773e+02, 3.34209e-01, 7.00000e+00],\n",
              "         [6.53951e+02, 1.25526e+01, 7.31177e+02, 7.07066e+01, 3.14578e-01, 2.00000e+00]]),\n",
              "   'masks': None,\n",
              "   'names': { 0: 'person',\n",
              "              1: 'bicycle',\n",
              "              2: 'car',\n",
              "              3: 'motorcycle',\n",
              "              4: 'airplane',\n",
              "              5: 'bus',\n",
              "              6: 'train',\n",
              "              7: 'truck',\n",
              "              8: 'boat',\n",
              "              9: 'traffic light',\n",
              "              10: 'fire hydrant',\n",
              "              11: 'stop sign',\n",
              "              12: 'parking meter',\n",
              "              13: 'bench',\n",
              "              14: 'bird',\n",
              "              15: 'cat',\n",
              "              16: 'dog',\n",
              "              17: 'horse',\n",
              "              18: 'sheep',\n",
              "              19: 'cow',\n",
              "              20: 'elephant',\n",
              "              21: 'bear',\n",
              "              22: 'zebra',\n",
              "              23: 'giraffe',\n",
              "              24: 'backpack',\n",
              "              25: 'umbrella',\n",
              "              26: 'handbag',\n",
              "              27: 'tie',\n",
              "              28: 'suitcase',\n",
              "              29: 'frisbee',\n",
              "              30: 'skis',\n",
              "              31: 'snowboard',\n",
              "              32: 'sports ball',\n",
              "              33: 'kite',\n",
              "              34: 'baseball bat',\n",
              "              35: 'baseball glove',\n",
              "              36: 'skateboard',\n",
              "              37: 'surfboard',\n",
              "              38: 'tennis racket',\n",
              "              39: 'bottle',\n",
              "              40: 'wine glass',\n",
              "              41: 'cup',\n",
              "              42: 'fork',\n",
              "              43: 'knife',\n",
              "              44: 'spoon',\n",
              "              45: 'bowl',\n",
              "              46: 'banana',\n",
              "              47: 'apple',\n",
              "              48: 'sandwich',\n",
              "              49: 'orange',\n",
              "              50: 'broccoli',\n",
              "              51: 'carrot',\n",
              "              52: 'hot dog',\n",
              "              53: 'pizza',\n",
              "              54: 'donut',\n",
              "              55: 'cake',\n",
              "              56: 'chair',\n",
              "              57: 'couch',\n",
              "              58: 'potted plant',\n",
              "              59: 'bed',\n",
              "              60: 'dining table',\n",
              "              61: 'toilet',\n",
              "              62: 'tv',\n",
              "              63: 'laptop',\n",
              "              64: 'mouse',\n",
              "              65: 'remote',\n",
              "              66: 'keyboard',\n",
              "              67: 'cell phone',\n",
              "              68: 'microwave',\n",
              "              69: 'oven',\n",
              "              70: 'toaster',\n",
              "              71: 'sink',\n",
              "              72: 'refrigerator',\n",
              "              73: 'book',\n",
              "              74: 'clock',\n",
              "              75: 'vase',\n",
              "              76: 'scissors',\n",
              "              77: 'teddy bear',\n",
              "              78: 'hair drier',\n",
              "              79: 'toothbrush'},\n",
              "   'orig_img': array([[[  4,   0,   3],\n",
              "         [ 30,  24,  29],\n",
              "         [ 24,  18,  23],\n",
              "         ...,\n",
              "         [ 13,  19,  14],\n",
              "         [ 10,  16,  11],\n",
              "         [  7,  13,   8]],\n",
              " \n",
              "        [[  5,   0,   4],\n",
              "         [ 25,  19,  24],\n",
              "         [ 34,  28,  33],\n",
              "         ...,\n",
              "         [  9,  15,  10],\n",
              "         [  8,  14,   9],\n",
              "         [  5,  11,   6]],\n",
              " \n",
              "        [[  6,   0,   5],\n",
              "         [  7,   1,   6],\n",
              "         [  8,   2,   7],\n",
              "         ...,\n",
              "         [  5,  11,   6],\n",
              "         [  5,  11,   6],\n",
              "         [  5,  11,   6]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 11,   2,   0],\n",
              "         [ 54,  42,  36],\n",
              "         [ 55,  34,  26],\n",
              "         ...,\n",
              "         [251, 247, 242],\n",
              "         [250, 246, 241],\n",
              "         [250, 246, 241]],\n",
              " \n",
              "        [[  9,   0,   0],\n",
              "         [ 42,  30,  24],\n",
              "         [ 38,  17,   9],\n",
              "         ...,\n",
              "         [250, 246, 241],\n",
              "         [249, 245, 240],\n",
              "         [249, 245, 240]],\n",
              " \n",
              "        [[ 12,   3,   0],\n",
              "         [ 40,  28,  22],\n",
              "         [ 36,  15,   7],\n",
              "         ...,\n",
              "         [250, 246, 241],\n",
              "         [249, 245, 240],\n",
              "         [249, 245, 240]]], dtype=uint8),\n",
              "   'orig_shape': (932, 1672),\n",
              "   'path': '/content/frame0.jpg',\n",
              "   'probs': None,\n",
              "   'speed': {'inference': 183.43067169189453, 'postprocess': 3.4608840942382812, 'preprocess': 1.3442039489746094}}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a model\n",
        "model = YOLO('yolov8n.yaml')  # build a new model from scratch\n",
        "\n",
        "# Use the model\n",
        "results = model.train(data='/content/data.yaml', epochs=15)  # train the model\n",
        "results = model.val()  # evaluate model performance on the validation set\n",
        "results = model('/content/frame0.jpg')  # predict on an image\n",
        "success = model.export(format='onnx')  # export the model to ONNX format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ue4ey1UegcjQ",
        "outputId": "bba7d28b-2145-41a3-9154-552c7860c999"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]          \n",
            "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
            "\n",
            "New https://pypi.org/project/ultralytics/8.0.57 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.0.56 🚀 Python-3.9.16 torch-1.13.1+cu116 CPU\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=/content/data.yaml, epochs=15, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ultralytics/yolo/engine/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, overrides)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.yaml'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'detect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'segment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_det_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'yaml_file'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ultralytics/yolo/data/utils.py\u001b[0m in \u001b[0;36mcheck_det_dataset\u001b[0;34m(dataset, autodownload)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: \nDataset '/content/data.yaml' images not found ⚠️, missing paths ['/content/datasets/data/valid/images']",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-25c54af50508>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Use the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/data.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# evaluate model performance on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/frame0.jpg'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# predict on an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ultralytics/yolo/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTASK_MAP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resume'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# manually set model only if not resuming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ultralytics/yolo/engine/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, overrides)\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yaml_file'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# for validating 'yolo train data=url.zip' usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memojis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{self.args.data}' error ❌ {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset '/content/data.yaml' error ❌ \nDataset '/content/data.yaml' images not found ⚠️, missing paths ['/content/datasets/data/valid/images']"
          ]
        }
      ]
    }
  ]
}